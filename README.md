# ğŸ§  Early Alzheimerâ€™s Detection from Speech Patterns (NLP + ML)

### Project Overview
This project aims to **detect early signs of Alzheimerâ€™s disease** using **speech and linguistic patterns**.  
We utilize the `addetector_dataset.csv` dataset containing **1010 samples** and **66 extracted features** â€” a mix of **acoustic (MFCCs)** and **linguistic embeddings**.

The final system can even take a **user-typed text input**, extract linguistic cues (like disfluency, coherence, and length), and **predict Alzheimerâ€™s likelihood**.

---

## ğŸ“˜ 1. Dataset Description

| Feature Group | Description |
|----------------|-------------|
| **duration_sec**, **chunk_count** | Speech length and fragmentation count |
| **mfcc_1 â€“ mfcc_13** | Acoustic speech features capturing tone and frequency |
| **linguistic_feat_1 â€“ linguistic_feat_50** | Linguistic embeddings / textual statistics |
| **label** | Target variable â€” `0 = Healthy`, `1 = Alzheimerâ€™s` |

---

## ğŸ§¹ 2. Clean Dataset

After preprocessing, we generate:

> `cleaned_addetector_dataset.csv`

This cleaned dataset will:
- Remove redundant or unnecessary columns  
- Scale features using StandardScaler  
- Handle missing/null values  
- Optionally reduce linguistic dimensions using **mean or PCA aggregation**

---

## âš™ï¸ 3. Workflow Overview

### **Notebook-Based Pipeline**
All tasks will be implemented within `.ipynb` notebooks for easier presentation and visualization.

**Notebooks:**
1. `01_Preprocessing.ipynb` â€” Cleaning, feature reduction, and dataset preparation  
2. `02_Model_Training.ipynb` â€” Ensemble + stacking models with hyperparameter tuning  
3. `03_Evaluation_and_Prediction.ipynb` â€” Final metrics, confusion matrix, and user text-based prediction  

---

## ğŸ§© 4. Feature Reduction Strategy

Since `linguistic_feat_1 â€“ linguistic_feat_50` are highly correlated embeddings:

### **Approach 1 â€” Mean Aggregation**
Compute one feature:
```python
df["linguistic_mean"] = df[[f"linguistic_feat_{i}" for i in range(1, 51)]].mean(axis=1)
```
âœ… Retains overall linguistic signal while reducing complexity.  
âœ… Works best for smaller datasets (like ours).

### **Approach 2 â€” PCA (Principal Component Analysis)**
Extract top components explaining 95% variance:
```python
from sklearn.decomposition import PCA
pca = PCA(n_components=0.95)
reduced_feats = pca.fit_transform(df[linguistic_features])
```
âœ… Keeps most information, reduces redundancy, ideal for model interpretability.

---

## ğŸ§  5. Model Selection

Weâ€™ll use **stacking and ensemble-based learning** for robustness.

| Model | Purpose |
|--------|----------|
| **Random Forest** | Handles nonlinear relations and feature importance |
| **XGBoost / LightGBM** | High performance with small datasets |
| **Logistic Regression** | Lightweight, interpretable baseline |
| **StackingClassifier** | Combines all above for the best F1 and ROC-AUC |

---

## ğŸ§ª 6. Hyperparameter Optimization

- Use **Optuna** or **GridSearchCV**
- Parameters tuned:
  - `max_depth`, `n_estimators`, `learning_rate` for XGBoost/LightGBM
  - `C`, `penalty` for Logistic Regression
  - `max_features`, `min_samples_split` for Random Forest

---

## ğŸ“ˆ 7. Model Training and Evaluation

Metrics:
- Accuracy  
- Precision / Recall / F1-score  
- ROC-AUC  
- Confusion Matrix  

Feature importance visualization will be done via **SHAP values** and **permutation importance**.

---

## âœï¸ 8. User Text-Based Prediction (Simulated Input)

Instead of real audio, users can **type a sentence**, which will be converted into a simplified **linguistic feature vector** using NLP preprocessing.

### Example Flow:
```python
Enter text: hi... ho ar u...

Predicted Output â†’ Alzheimer's Detected
Confidence â†’ 0.89
```

### How It Works:
1. The text is analyzed using NLP:
   - Sentence length
   - Pauses (â€œ...â€ count)
   - Word diversity
   - Grammatical completeness
   - Average word length
2. These linguistic patterns are transformed into a numerical feature vector.
3. The trained ensemble model predicts whether the text reflects **healthy or Alzheimer-like** linguistic patterns.

---

## ğŸ“‚ 9. Folder Structure

```
AlzheimerSpeechDetection/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ addetector_dataset.csv
â”‚   â””â”€â”€ cleaned_addetector_dataset.csv
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_Preprocessing.ipynb
â”‚   â”œâ”€â”€ 02_Model_Training.ipynb
â”‚   â”œâ”€â”€ 03_Evaluation_and_Prediction.ipynb
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ final_model.pkl
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ metrics.json
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â””â”€â”€ feature_importance.png
â”‚
â””â”€â”€ README.md
```

---

## ğŸ§° 10. Tech Stack

- **Language:** Python 3.10+
- **Libraries:**
  - pandas, numpy, scikit-learn  
  - xgboost, lightgbm, optuna  
  - shap, matplotlib, seaborn  
  - nltk, textstat (for text-based user input)

---

## ğŸ¯ 11. Final Output

- A **stacked ensemble classifier** that predicts Alzheimerâ€™s vs Healthy speech patterns.  
- A **text-input prediction cell** allowing real-time evaluation.  
- Clean, reproducible `.ipynb` notebooks for presentation and model interpretation.

---

## ğŸš€ 12. Future Enhancements

- Extend to real audio input via automatic speech recognition (ASR).  
- Fine-tune transformer models (BERT-based linguistic embedding).  
- Build a multi-class cognitive detection model (Normal / MCI / Alzheimerâ€™s).  
